{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>\n",
    "ref: https://i.imgur.com/w395Yk9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1.LSTM after removing the Low and High idf value words. (In model-1 Trained on total data but in Model-2 trained on data after removing some words based on IDF values)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/fkQ8nGo.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing all required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Conv1D, MaxPooling1D,concatenate,BatchNormalization,Activation,Dropout,Input,Flatten,Embedding,LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109248, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.read_csv(\"lstm_preprocessed_data.csv\")\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"final_neg = final[final[\"project_is_approved\"] == 0]\n",
    "final_pos = final[final[\"project_is_approved\"] == 1]\n",
    "\n",
    "lst_ran = []\n",
    "for i in range(10000):\n",
    "    lst_ran.append(final_neg.iloc[i])\n",
    "    lst_ran.append(final_pos.iloc[-i])\n",
    "    \n",
    "project_data = pd.DataFrame(lst_ran)\n",
    "project_data.shape\"\"\"\n",
    "project_data = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "\n",
       "  clean_categories                 clean_subcategories  \\\n",
       "0     math_science  appliedsciences health_lifescience   \n",
       "1     specialneeds                        specialneeds   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data (87398, 9)\n",
      "--------------------------------------------------\n",
      "Number of data points in test data (21850, 9)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "project_data_train, project_data_test, y_train, y_test = train_test_split(project_data, project_data[\"project_is_approved\"], test_size=0.2, stratify=project_data[\"project_is_approved\"], random_state = 0)\n",
    "\n",
    "print(\"Number of data points in train data\", project_data_train.shape)\n",
    "print('-'*50)\n",
    "print(\"Number of data points in test data\", project_data_test.shape)\n",
    "print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = dict(project_data_train[\"clean_categories\"].value_counts())\n",
    "val_d = dict(enumerate(val.keys()))\n",
    "category_val = {v: k for k, v in val_d.items()}\n",
    "\n",
    "cate_train = project_data_train[\"clean_categories\"].values\n",
    "category_train = []\n",
    "for i in  cate_train:\n",
    "    category_train.append(category_val[i])\n",
    "    \n",
    "cate_test = project_data_test[\"clean_categories\"].values\n",
    "category_test = []\n",
    "for i in  cate_test:\n",
    "    if i in category_val:\n",
    "        category_test.append(category_val[i])\n",
    "    else:\n",
    "        category_test.append(len(val_d.keys()))\n",
    "\n",
    "category_train = np.array(category_train)\n",
    "category_test = np.array(category_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = dict(project_data_train[\"clean_subcategories\"].value_counts())\n",
    "val_d = dict(enumerate(val.keys()))\n",
    "subcategory_val = {v: k for k, v in val_d.items()}\n",
    "\n",
    "subcate_train = project_data_train[\"clean_subcategories\"].values\n",
    "subcategory_train = []\n",
    "for i in  subcate_train:\n",
    "    subcategory_train.append(subcategory_val[i])\n",
    "    \n",
    "subcate_test = project_data_test[\"clean_subcategories\"].values\n",
    "subcategory_test = []\n",
    "for i in  subcate_test:\n",
    "    if i in subcategory_val:\n",
    "        subcategory_test.append(subcategory_val[i])\n",
    "    else:\n",
    "        subcategory_test.append(len(val_d.keys()))\n",
    "        \n",
    "subcategory_train = np.array(subcategory_train)\n",
    "subcategory_test = np.array(subcategory_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = dict(project_data_train[\"school_state\"].value_counts())\n",
    "val_d = dict(enumerate(val.keys()))\n",
    "state_val = {v: k for k, v in val_d.items()}\n",
    "\n",
    "state_tr = project_data_train[\"school_state\"].values\n",
    "state_enc_train = []\n",
    "for i in  state_tr:\n",
    "    state_enc_train.append(state_val[i])\n",
    "    \n",
    "state_te = project_data_test[\"school_state\"].values\n",
    "state_enc_test = []\n",
    "for i in  state_te:\n",
    "    if i in state_val:\n",
    "        state_enc_test.append(state_val[i])\n",
    "    else:\n",
    "        state_enc_test.append(len(val_d.keys()))\n",
    "        \n",
    "state_enc_train = np.array(state_enc_train)\n",
    "state_enc_test = np.array(state_enc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = dict(project_data_train[\"project_grade_category\"].value_counts())\n",
    "val_d = dict(enumerate(val.keys()))\n",
    "grade_val = {v: k for k, v in val_d.items()}\n",
    "\n",
    "grade_train = project_data_train[\"project_grade_category\"].values\n",
    "grade_enc_train = []\n",
    "for i in  grade_train:\n",
    "    grade_enc_train.append(grade_val[i])\n",
    "    \n",
    "grade_test = project_data_test[\"project_grade_category\"].values\n",
    "grade_enc_test = []\n",
    "for i in  grade_test:\n",
    "    if i in grade_val:\n",
    "        grade_enc_test.append(grade_val[i])\n",
    "    else:\n",
    "        grade_enc_test.append(len(val_d.keys()))\n",
    "        \n",
    "grade_enc_train = np.array(grade_enc_train)\n",
    "grade_enc_test = np.array(grade_enc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = dict(project_data_train[\"teacher_prefix\"].value_counts())\n",
    "val_d = dict(enumerate(val.keys()))\n",
    "prefix_val = {v: k for k, v in val_d.items()}\n",
    "\n",
    "prefix_train = project_data_train[\"teacher_prefix\"].values\n",
    "prefix_enc_train = []\n",
    "for i in  prefix_train:\n",
    "    prefix_enc_train.append(prefix_val[i])\n",
    "    \n",
    "prefix_test = project_data_test[\"teacher_prefix\"].values\n",
    "prefix_enc_test = []\n",
    "for i in  prefix_test:\n",
    "    if i in prefix_val:\n",
    "        prefix_enc_test.append(prefix_val[i])\n",
    "    else:\n",
    "        prefix_enc_test.append(len(val_d.keys()))\n",
    "        \n",
    "prefix_enc_train = np.array(prefix_enc_train)\n",
    "prefix_enc_test = np.array(prefix_enc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "essay_train = project_data_train[\"essay\"].values\n",
    "essay_test = project_data_test[\"essay\"].values\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(essay_train)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[175, 20, 23, 2131, 53, 80, 14, 491, 312, 10, 1600, 457, 3729, 116, 10, 25, 1510, 292, 1, 816, 3, 30, 182, 840, 344, 4, 1, 6163, 1263, 547, 142, 1897, 4, 1, 1853, 1176, 880, 10, 14, 2480, 391, 160, 284, 4, 1, 40, 67, 26, 6, 1480, 1603, 463, 1824, 78, 6, 1441, 14, 1441, 44, 88, 102, 9, 3465, 14, 46, 32, 8818, 346, 8986, 38, 32, 101, 46, 827, 32, 6, 167, 98, 33016, 54, 2, 34, 153, 2720, 3254, 3692, 231, 487, 376, 32, 2, 34, 1, 407, 538, 5053, 2705, 57, 118, 14, 15, 2852, 33, 1519, 14, 115, 5, 33, 2852, 193, 4, 1, 28, 15, 2346, 2519, 664, 1894, 3649, 530, 142, 13]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "#Train\n",
    "enc_docs_train = t.texts_to_sequences(essay_train)\n",
    "print(enc_docs_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 492, 20, 70, 26, 120, 864, 146, 4, 1, 19, 6164, 968, 624, 65, 1052, 118, 4578, 6, 7897, 2433, 161, 194, 20, 14, 19, 792, 73, 854, 109, 686, 14, 19, 17, 337, 23, 767, 1236, 74, 102, 241, 32, 69, 4, 23, 26, 373, 6, 4280, 14, 347, 50, 132, 138, 1, 169, 67, 273, 414, 680, 206, 3, 115, 216, 352, 623, 39, 1, 295, 48, 429, 6173, 110, 65, 41, 26, 638, 1047, 944, 1, 67, 134, 1526, 422, 125, 26, 38, 118, 791, 101, 160, 4318, 68, 309, 242, 207, 472, 1519, 4467, 4368, 1212, 3172, 19404, 50, 330, 32, 1941, 288, 1, 25, 23, 46, 32, 131, 2491, 303, 1941, 288, 68, 309, 203, 255, 1222, 1184, 32, 28, 49, 1313, 186, 2254, 3771, 202, 2712, 45, 15, 186, 2375, 159, 27, 100, 15, 10, 17, 102, 2, 89, 40, 3618, 1950, 23, 46, 32, 131, 13]\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "enc_docs_test = t.texts_to_sequences(essay_test)\n",
    "print(enc_docs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 400)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    2   49 1520 1637  380 1153   22    3   20 1009   89  160    6\n",
      "  229   14  133  157  719  208   25   23 1231 3501   14 3475   10  491\n",
      "   19 2227  817  208    4    3  113  184  335  617  867    1  134   64\n",
      "   81  221    1  686  161  525    8    2  137  433  659   33   44   37\n",
      "   62  137   53   10  225  197   84    4  142  719   22    6  208   89\n",
      "   10  198  317  519  158  320  143 1779  174  666  417   44   35  280\n",
      "  714   16  307    1  666  417  754  739    1  697   21  174   52  112\n",
      "  786  420 1607 1679 1056 1008  725  249  112  163  217 1493   11   12\n",
      "   62 2833  248 1257  123  587 4419  409    2   94 1740   49  261  157\n",
      "   76 1892  249  112    6  525 3264  319   90  801   39  503  249  112\n",
      "  282   11    1   40  249  112  282   13]\n"
     ]
    }
   ],
   "source": [
    "# truncate and/or pad input sequences\n",
    "max_review_length = 400\n",
    "pad_docs_train = pad_sequences(enc_docs_train, maxlen=max_review_length)\n",
    "pad_docs_test = pad_sequences(enc_docs_test, maxlen=max_review_length)\n",
    "\n",
    "print(pad_docs_train.shape)\n",
    "print(pad_docs_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 1917495  words loaded!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "glovemodel = loadGloveModel('glove.42B.300d.txt')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = glovemodel.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 298.3417191468912, Standard deviation : 365.8936474660223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "price_scalar = StandardScaler()\n",
    "price_scalar.fit(project_data_train['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n",
    "print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n",
    "\n",
    "# Now standardize the data with above maen and variance.\n",
    "price_standardized = price_scalar.transform(project_data_train['price'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 298.3417191468912, Standard deviation : 365.8936474660223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "price_scalar = StandardScaler()\n",
    "price_scalar.fit(project_data_train['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n",
    "print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n",
    "\n",
    "# Now standardize the data with above maen and variance.\n",
    "price_standardized_test = price_scalar.transform(project_data_test['price'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 11.131524748850088, Standard deviation : 27.733763561081116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "previouspro_scalar = StandardScaler()\n",
    "previouspro_scalar.fit(project_data_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n",
    "print(f\"Mean : {previouspro_scalar.mean_[0]}, Standard deviation : {np.sqrt(previouspro_scalar.var_[0])}\")\n",
    "\n",
    "\n",
    "previouspro_standardized_train = previouspro_scalar.transform(project_data_train['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 11.131524748850088, Standard deviation : 27.733763561081116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "previouspro_scalar = StandardScaler()\n",
    "previouspro_scalar.fit(project_data_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n",
    "print(f\"Mean : {previouspro_scalar.mean_[0]}, Standard deviation : {np.sqrt(previouspro_scalar.var_[0])}\")\n",
    "\n",
    "\n",
    "previouspro_standardized_test = previouspro_scalar.transform(project_data_test['teacher_number_of_previously_posted_projects'].values.reshape(-1, 1))\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_train = np.stack((price_standardized, previouspro_standardized_train), axis=-1)\n",
    "remain_train = remain_train.reshape(remain_train.shape[0], remain_train.shape[1]*remain_train.shape[2])\n",
    "remain_test = np.stack((price_standardized_test, previouspro_standardized_test), axis=-1)\n",
    "remain_test = remain_test.reshape(remain_test.shape[0], remain_test.shape[1]*remain_test.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Input_model_1 = Input(shape=(max_review_length,) ,name=\"Text_data\")\n",
    "layer11 = Embedding(vocab_size, 300 ,weights=[embedding_matrix],trainable=False ,name=\"layer11\")(Input_model_1)\n",
    "layer21 = LSTM(units = 100,activation='relu',kernel_initializer='he_normal',return_sequences = False,name=\"layer21\")(layer11)\n",
    "layer31 = Flatten()(layer21)\n",
    "\n",
    "Input_model_2 = Input(shape=(1,),name=\"School_state\")\n",
    "layer12 = Embedding(len(state_val) + 1, 32 ,name=\"layer12\")(Input_model_2)\n",
    "layer22 = Flatten()(layer12)\n",
    "\n",
    "Input_model_3 = Input(shape=(1,),name=\"Grade\")\n",
    "layer13 = Embedding(len(grade_val) + 1, 32 ,name=\"layer13\")(Input_model_3)\n",
    "layer23 = Flatten()(layer13)\n",
    "\n",
    "Input_model_4 = Input(shape=(1,),name=\"Categories\")\n",
    "layer14 = Embedding(len(category_val) + 1, 32 ,name=\"layer14\")(Input_model_4)\n",
    "layer24 = Flatten()(layer14)\n",
    "\n",
    "Input_model_5 = Input(shape=(1,),name=\"sub_Category\")\n",
    "layer15 = Embedding(len(subcategory_val) + 1, 32 ,name=\"layer15\")(Input_model_5)\n",
    "layer25 = Flatten()(layer15)\n",
    "\n",
    "Input_model_6 = Input(shape=(1,),name=\"Prefix\")\n",
    "layer16 = Embedding(len(prefix_val) + 1, 32 ,name=\"layer16\")(Input_model_6)\n",
    "layer26 = Flatten()(layer16)\n",
    "\n",
    "Input_model_7 = Input(shape=(2,),name=\"Remaining_features\")\n",
    "layer27 = Dense(units=8,activation='relu',kernel_initializer=\"he_normal\",name=\"layer27\")(Input_model_7)\n",
    "\n",
    "concat_layer = concatenate(inputs=[layer31,layer22,layer23,layer24,layer25,layer26,layer27],name=\"concat\")\n",
    "\n",
    "layer2 = Dense(units=512,activation='relu',kernel_initializer='he_normal',name=\"layer2\")(concat_layer)\n",
    "norm_1 = BatchNormalization()(layer2)\n",
    "layer3 = Dropout(0.25)(norm_1)\n",
    "layer4 = Dense(units=256,activation='relu',kernel_initializer='he_normal',name=\"layer4\")(layer3)\n",
    "norm_2 = BatchNormalization()(layer4)\n",
    "layer5 = Dense(units=128,activation='relu',kernel_initializer='he_normal',name=\"layer5\")(norm_2)\n",
    "norm_3 = BatchNormalization()(layer5)\n",
    "layer6 = Dense(units=64,activation='relu',kernel_initializer='he_normal',name=\"layer6\")(norm_3)\n",
    "\n",
    "output = Dense(units=1,activation='sigmoid',kernel_initializer=\"glorot_uniform\",name=\"output\")(layer6)\n",
    "\n",
    "model = Model(inputs=[Input_model_1,Input_model_2,Input_model_3,Input_model_4,Input_model_5,Input_model_6,Input_model_7],outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Text_data (InputLayer)          [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer11 (Embedding)             (None, 400, 300)     15540900    Text_data[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "School_state (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Grade (InputLayer)              [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Categories (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sub_Category (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Prefix (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer21 (LSTM)                  (None, 100)          160400      layer11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer12 (Embedding)             (None, 1, 32)        1664        School_state[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer13 (Embedding)             (None, 1, 32)        160         Grade[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer14 (Embedding)             (None, 1, 32)        1664        Categories[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer15 (Embedding)             (None, 1, 32)        12608       sub_Category[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer16 (Embedding)             (None, 1, 32)        192         Prefix[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Remaining_features (InputLayer) [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           layer21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 32)           0           layer12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 32)           0           layer13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 32)           0           layer14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 32)           0           layer15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 32)           0           layer16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer27 (Dense)                 (None, 8)            24          Remaining_features[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 268)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 layer27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 512)          137728      concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512)          2048        layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer4 (Dense)                  (None, 256)          131328      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        layer4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer5 (Dense)                  (None, 128)          32896       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         layer5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer6 (Dense)                  (None, 64)           8256        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            65          layer6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 16,031,469\n",
      "Trainable params: 488,777\n",
      "Non-trainable params: 15,542,692\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile \n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics = [\"accuracy\", auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class_weights = {0 : 1,\\n                 1 :0.2}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class_weights = {0 : 1,\n",
    "                 1 :0.2}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "logs = r\"C:\\Users\\Dewang\\Desktop\\AAIC notes\\0.0 Assignments\\AAIC classroom Assignments\\logs\\ex1\"\n",
    "tensorboard = TensorBoard(log_dir = logs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "87398/87398 [==============================] - 577s 7ms/sample - loss: 0.4248 - acc: 0.8480 - auc: 0.5906 - val_loss: 0.4334 - val_acc: 0.8486 - val_auc: 0.4089\n",
      "Epoch 2/10\n",
      "87398/87398 [==============================] - 504s 6ms/sample - loss: 0.4169 - acc: 0.8485 - auc: 0.6133 - val_loss: 0.4318 - val_acc: 0.8486 - val_auc: 0.4115\n",
      "Epoch 3/10\n",
      "87398/87398 [==============================] - 504s 6ms/sample - loss: 0.4134 - acc: 0.8486 - auc: 0.6260 - val_loss: 0.4389 - val_acc: 0.8486 - val_auc: 0.4212\n",
      "Epoch 4/10\n",
      "87398/87398 [==============================] - 504s 6ms/sample - loss: 0.4115 - acc: 0.8485 - auc: 0.6354 - val_loss: 0.4345 - val_acc: 0.8486 - val_auc: 0.4400\n",
      "Epoch 5/10\n",
      "87398/87398 [==============================] - 504s 6ms/sample - loss: 0.4094 - acc: 0.8486 - auc: 0.6408 - val_loss: 0.4292 - val_acc: 0.8486 - val_auc: 0.5081\n",
      "Epoch 6/10\n",
      "87398/87398 [==============================] - 503s 6ms/sample - loss: 0.4076 - acc: 0.8488 - auc: 0.6480 - val_loss: 0.4304 - val_acc: 0.8486 - val_auc: 0.5037\n",
      "Epoch 7/10\n",
      "87398/87398 [==============================] - 504s 6ms/sample - loss: 0.4058 - acc: 0.8488 - auc: 0.6546 - val_loss: 0.4263 - val_acc: 0.8486 - val_auc: 0.5504\n",
      "Epoch 8/10\n",
      "87398/87398 [==============================] - 504s 6ms/sample - loss: 0.4035 - acc: 0.8493 - auc: 0.6623 - val_loss: 0.4239 - val_acc: 0.8486 - val_auc: 0.5406\n",
      "Epoch 9/10\n",
      "87398/87398 [==============================] - 506s 6ms/sample - loss: 0.4010 - acc: 0.8496 - auc: 0.6694 - val_loss: 0.4246 - val_acc: 0.8486 - val_auc: 0.5396\n",
      "Epoch 10/10\n",
      "87398/87398 [==============================] - 506s 6ms/sample - loss: 0.3980 - acc: 0.8501 - auc: 0.6783 - val_loss: 0.4350 - val_acc: 0.8486 - val_auc: 0.4985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c0cf7d4ef0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "model.fit([pad_docs_train,state_enc_train,grade_enc_train,category_train,subcategory_train,prefix_enc_train,remain_train],y_train,\n",
    "           batch_size=256,epochs=10,\n",
    "           validation_data= ([pad_docs_test,state_enc_test,grade_enc_test,category_test,subcategory_test,prefix_enc_test,remain_test],y_test),\n",
    "          callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"model1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1011 12:07:16.377816  8056 deprecation.py:506] From C:\\Users\\Dewang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1011 12:07:16.409270  8056 deprecation.py:506] From C:\\Users\\Dewang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1011 12:07:16.429961  8056 deprecation.py:506] From C:\\Users\\Dewang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1011 12:07:16.437951  8056 deprecation.py:506] From C:\\Users\\Dewang\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "json_file = open('model1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model1.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile \n",
    "loaded_model.compile(optimizer='adam',loss='binary_crossentropy',metrics = [\"accuracy\", auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21850/21850 [==============================] - 177s 8ms/sample - loss: 0.5534 - acc: 0.7437 - auc: 0.7261\n",
      "Test score: 0.5534265434059998\n",
      "Test accuracy: 0.7437071\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate([pad_docs_test,state_enc_test,grade_enc_test,category_test,subcategory_test,prefix_enc_test,remain_test],y_test,batch_size = 64, verbose=1) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 810 (pid 10556), started 0:00:45 ago. (Use '!kill 10556' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:810\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x24365001c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs --host localhost --port 810"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "essay_train = project_data_train[\"essay\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(essay_train)\n",
    "idf = vectorizer.idf_\n",
    "idf_dict = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "idf_dict[\"i\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idf_dict = dict(sorted(idf_dict.items(), key=lambda kv: kv[1]))\n",
    "sorted_idf = list(sorted_idf_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20a69ed67b8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(y = sorted_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Removing Values below 6 and above 10\"\"\"\n",
    "remove_words = {}\n",
    "for i,j in idf_dict.items():\n",
    "    if(int(j)<6 or int(j) >10):\n",
    "        remove_words[i] = idf_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Removing Words from Data\\nfrom tqdm import tqdm\\n\\nessay_train = project_data_train[\"essay\"]\\nessay_test = project_data_test[\"essay\"]\\n\\nstopwords = list(remove_words.keys())\\n\\nidf_essay_train = []\\n\\nfor query in tqdm(essay_train):\\n    \\n    querywords = query.split()\\n    resultwords  = [word for word in querywords if word.lower() not in stopwords]\\n    result = \\' \\'.join(resultwords)\\n    idf_essay_train.append(result)\\n    \\n    \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Removing Words from Data\n",
    "from tqdm import tqdm\n",
    "\n",
    "essay_train = project_data_train[\"essay\"]\n",
    "essay_test = project_data_test[\"essay\"]\n",
    "\n",
    "stopwords = list(remove_words.keys())\n",
    "\n",
    "idf_essay_train = []\n",
    "\n",
    "for query in tqdm(essay_train):\n",
    "    \n",
    "    querywords = query.split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "    result = ' '.join(resultwords)\n",
    "    idf_essay_train.append(result)\n",
    "    \n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"idf_essay_test = []\n",
    "\n",
    "for query in tqdm(essay_test):\n",
    "    \n",
    "    querywords = query.split()\n",
    "    resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
    "    result = ' '.join(resultwords)\n",
    "    idf_essay_test.append(result)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df2_train = pd.DataFrame(idf_essay_train,columns = [\"train_essay\"])\n",
    "df2_test = pd.DataFrame(idf_essay_test,columns = [\"test_essay\"])\n",
    "df2_train.to_csv(r'lstm_model2_train.csv')\n",
    "df2_test.to_csv(r\"lstm_model2_test.csv\")\"\"\"\n",
    "\n",
    "df2_train = pd.read_csv(\"lstm_model2_train.csv\")\n",
    "df2_test = pd.read_csv(\"lstm_model2_test.csv\")\n",
    "\n",
    "idf_essay_train= df2_train[\"train_essay\"].astype(str)\n",
    "idf_essay_test= df2_test[\"test_essay\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(idf_essay_train)\n",
    "vocab_size_idf = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2360, 3424, 3381, 4526, 244]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "#Train\n",
    "idf_docs_train = t.texts_to_sequences(idf_essay_train)\n",
    "print(idf_docs_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[370, 9405, 613, 613, 613, 370, 2857, 15363, 731, 987, 12924, 370, 836, 2937, 21536, 2453, 1561, 399, 278, 1773, 2163, 1147, 4067, 1346, 1147, 2761, 21536, 15181, 170]\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "idf_docs_test = t.texts_to_sequences(idf_essay_test)\n",
    "print(idf_docs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 200)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0  414  784  920\n",
      " 1525  542    1    3]\n"
     ]
    }
   ],
   "source": [
    "# truncate and/or pad input sequences\n",
    "max_review_length = 200\n",
    "pad_idf_train = pad_sequences(idf_docs_train, maxlen=max_review_length)\n",
    "pad_idf_test = pad_sequences(idf_docs_test, maxlen=max_review_length)\n",
    "\n",
    "print(pad_idf_train.shape)\n",
    "print(pad_idf_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_idf = np.zeros((vocab_size_idf, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = glovemodel.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_idf[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Text_data (InputLayer)          [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer11 (Embedding)             (None, 400, 300)     7017000     Text_data[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "School_state (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Grade (InputLayer)              [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Categories (InputLayer)         [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sub_Category (InputLayer)       [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Prefix (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer21 (LSTM)                  (None, 100)          160400      layer11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer12 (Embedding)             (None, 1, 2)         104         School_state[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer13 (Embedding)             (None, 3, 2)         10          Grade[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer14 (Embedding)             (None, 5, 4)         40          Categories[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer15 (Embedding)             (None, 6, 4)         124         sub_Category[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "layer16 (Embedding)             (None, 1, 2)         12          Prefix[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "Remaining_features (InputLayer) [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           layer21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2)            0           layer12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 6)            0           layer13[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 20)           0           layer14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 24)           0           layer15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2)            0           layer16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer27 (Dense)                 (None, 8)            24          Remaining_features[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 162)          0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 layer27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 512)          83456       concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512)          2048        layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer4 (Dense)                  (None, 256)          131328      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        layer4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer5 (Dense)                  (None, 128)          32896       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         layer5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer6 (Dense)                  (None, 64)           8256        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            65          layer6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,437,299\n",
      "Trainable params: 418,507\n",
      "Non-trainable params: 7,018,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Input_model_1 = Input(shape=(max_review_length,) ,name=\"Text_data\")\n",
    "layer11 = Embedding(vocab_size_idf, 300 ,weights=[embedding_matrix_idf],trainable=False ,name=\"layer11\")(Input_model_1)\n",
    "layer21 = LSTM(units = 100,activation='relu',kernel_initializer='he_normal',return_sequences = False,name=\"layer21\")(layer11)\n",
    "layer31 = Flatten()(layer21)\n",
    "\n",
    "Input_model_2 = Input(shape=(1,),name=\"School_state\")\n",
    "layer12 = Embedding(state_size + 1, 2 ,name=\"layer12\")(Input_model_2)\n",
    "layer22 = Flatten()(layer12)\n",
    "\n",
    "Input_model_3 = Input(shape=(3,),name=\"Grade\")\n",
    "layer13 = Embedding(grade_size + 1, 2 ,name=\"layer13\")(Input_model_3)\n",
    "layer23 = Flatten()(layer13)\n",
    "\n",
    "Input_model_4 = Input(shape=(max_category_length,),name=\"Categories\")\n",
    "layer14 = Embedding(cate_size + 1, 4 ,name=\"layer14\")(Input_model_4)\n",
    "layer24 = Flatten()(layer14)\n",
    "\n",
    "Input_model_5 = Input(shape=(max_subcategory_length,),name=\"sub_Category\")\n",
    "layer15 = Embedding(subcate_size + 1, 4 ,name=\"layer15\")(Input_model_5)\n",
    "layer25 = Flatten()(layer15)\n",
    "\n",
    "Input_model_6 = Input(shape=(1,),name=\"Prefix\")\n",
    "layer16 = Embedding(prefix_size + 1, 2 ,name=\"layer16\")(Input_model_6)\n",
    "layer26 = Flatten()(layer16)\n",
    "\n",
    "Input_model_7 = Input(shape=(2,),name=\"Remaining_features\")\n",
    "layer27 = Dense(units=8,activation='relu',kernel_initializer=\"he_normal\",name=\"layer27\")(Input_model_7)\n",
    "\n",
    "concat_layer = concatenate(inputs=[layer31,layer22,layer23,layer24,layer25,layer26,layer27],name=\"concat\")\n",
    "\n",
    "layer2 = Dense(units=512,activation='relu',kernel_initializer='he_normal',name=\"layer2\")(concat_layer)\n",
    "norm_1 = BatchNormalization()(layer2)\n",
    "layer3 = Dropout(0.25)(norm_1)\n",
    "layer4 = Dense(units=256,activation='relu',kernel_initializer='he_normal',name=\"layer4\")(layer3)\n",
    "norm_2 = BatchNormalization()(layer4)\n",
    "layer5 = Dense(units=128,activation='relu',kernel_initializer='he_normal',name=\"layer5\")(norm_2)\n",
    "norm_3 = BatchNormalization()(layer5)\n",
    "layer6 = Dense(units=64,activation='relu',kernel_initializer='he_normal',name=\"layer6\")(norm_3)\n",
    "\n",
    "output = Dense(units=1,activation='sigmoid',kernel_initializer=\"glorot_uniform\",name=\"output\")(layer6)\n",
    "\n",
    "model = Model(inputs=[Input_model_1,Input_model_2,Input_model_3,Input_model_4,Input_model_5,Input_model_6,Input_model_7],outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile \n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics = [\"accuracy\", auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "logs1 = r\"C:\\Users\\Dewang\\Desktop\\AAIC notes\\0.0 Assignments\\AAIC classroom Assignments\\logs\\ex2\"\n",
    "tensorboard = TensorBoard(log_dir = logs1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1,\n",
    "                1: 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "87398/87398 [==============================] - 273s 3ms/sample - loss: 0.2299 - acc: 0.6048 - auc: 0.5190 - val_loss: 0.6576 - val_acc: 0.8486 - val_auc: 0.5000\n",
      "Epoch 2/10\n",
      "87398/87398 [==============================] - 271s 3ms/sample - loss: 0.2246 - acc: 0.6398 - auc: 0.5180 - val_loss: 0.6976 - val_acc: 0.1514 - val_auc: 0.5002\n",
      "Epoch 3/10\n",
      "87398/87398 [==============================] - 271s 3ms/sample - loss: 0.2232 - acc: 0.6618 - auc: 0.5192 - val_loss: 0.6093 - val_acc: 0.8486 - val_auc: 0.4998\n",
      "Epoch 4/10\n",
      "87398/87398 [==============================] - 271s 3ms/sample - loss: 0.2219 - acc: 0.6815 - auc: 0.5323 - val_loss: 0.6406 - val_acc: 0.8486 - val_auc: 0.5000\n",
      "Epoch 5/10\n",
      "87398/87398 [==============================] - 270s 3ms/sample - loss: 0.2210 - acc: 0.6997 - auc: 0.5450 - val_loss: 0.6357 - val_acc: 0.8486 - val_auc: 0.5000\n",
      "Epoch 6/10\n",
      "87398/87398 [==============================] - 273s 3ms/sample - loss: 0.2204 - acc: 0.6919 - auc: 0.5499 - val_loss: 0.6409 - val_acc: 0.8486 - val_auc: 0.5000\n",
      "Epoch 7/10\n",
      "87398/87398 [==============================] - 308s 4ms/sample - loss: 0.2204 - acc: 0.7116 - auc: 0.5489 - val_loss: 0.7018 - val_acc: 0.1514 - val_auc: 0.5000\n",
      "Epoch 8/10\n",
      "87398/87398 [==============================] - 301s 3ms/sample - loss: 0.2201 - acc: 0.7057 - auc: 0.5512 - val_loss: 0.6591 - val_acc: 0.8486 - val_auc: 0.5003\n",
      "Epoch 9/10\n",
      "87398/87398 [==============================] - 261s 3ms/sample - loss: 0.2198 - acc: 0.7297 - auc: 0.5562 - val_loss: 0.6475 - val_acc: 0.8486 - val_auc: 0.5000\n",
      "Epoch 10/10\n",
      "87398/87398 [==============================] - 260s 3ms/sample - loss: 0.2195 - acc: 0.7385 - auc: 0.5590 - val_loss: 0.6431 - val_acc: 0.8486 - val_auc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2434efff898>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "model.fit([pad_idf_train,state_enc_train,grade_enc_train,category_train,subcategory_train,prefix_enc_train,remain_train],y_train,\n",
    "           batch_size=256,epochs=10,\n",
    "           validation_data= ([pad_idf_test,state_enc_test,grade_enc_test,category_test,subcategory_test,prefix_enc_test,remain_test],y_test),\n",
    "           class_weight=class_weights,callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"model2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "json_file = open('model2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model2.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile \n",
    "loaded_model.compile(optimizer='adam',loss='binary_crossentropy',metrics = [\"accuracy\", auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600/9600 [==============================] - 73s 8ms/sample - loss: 0.6638 - acc: 0.6085 - auc: 0.6357\n",
      "Test score: 0.6637970036268235\n",
      "Test accuracy: 0.60854167\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate([pad_idf_test,state_enc_test,grade_enc_test,category_test,subcategory_test,prefix_enc_test,remain_test],y_test,\n",
    "           callbacks = [tensorboard])\n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8089 (pid 11512), started 0:01:58 ago. (Use '!kill 11512' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:8089\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x21cc1a8d048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs1 --host localhost --port 8089"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorica Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appliedlearning', 'care_hunger', 'health_sports', 'history_civics', 'literacy_language', 'math_science', 'music_arts', 'specialneeds', 'warmth']\n",
      "Shape of matrix after one hot encodig  (87398, 9)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "categories_one_hot = vectorizer.fit_transform(project_data_train['clean_categories'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix after one hot encodig \",categories_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appliedlearning', 'care_hunger', 'health_sports', 'history_civics', 'literacy_language', 'math_science', 'music_arts', 'specialneeds', 'warmth']\n",
      "Shape of matrix after one hot encodig  (21850, 9)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(project_data_train['clean_categories'].values)\n",
    "categories_one_hot_test = vectorizer.transform(project_data_test['clean_categories'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix after one hot encodig \",categories_one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appliedsciences', 'care_hunger', 'charactereducation', 'civics_government', 'college_careerprep', 'communityservice', 'earlydevelopment', 'economics', 'environmentalscience', 'esl', 'extracurricular', 'financialliteracy', 'foreignlanguages', 'gym_fitness', 'health_lifescience', 'health_wellness', 'history_geography', 'literacy', 'literature_writing', 'mathematics', 'music', 'nutritioneducation', 'other', 'parentinvolvement', 'performingarts', 'socialsciences', 'specialneeds', 'teamsports', 'visualarts', 'warmth']\n",
      "Shape of matrix after one hot encodig  (87398, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "vectorizer = CountVectorizer()\n",
    "sub_categories_one_hot = vectorizer.fit_transform(project_data_train['clean_subcategories'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix after one hot encodig \",sub_categories_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appliedsciences', 'care_hunger', 'charactereducation', 'civics_government', 'college_careerprep', 'communityservice', 'earlydevelopment', 'economics', 'environmentalscience', 'esl', 'extracurricular', 'financialliteracy', 'foreignlanguages', 'gym_fitness', 'health_lifescience', 'health_wellness', 'history_geography', 'literacy', 'literature_writing', 'mathematics', 'music', 'nutritioneducation', 'other', 'parentinvolvement', 'performingarts', 'socialsciences', 'specialneeds', 'teamsports', 'visualarts', 'warmth']\n",
      "Shape of matrix after one hot encodig  (21850, 30)\n"
     ]
    }
   ],
   "source": [
    "# we use count vectorizer to convert the values into one \n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(project_data_train['clean_subcategories'].values)\n",
    "sub_categories_one_hot_test = vectorizer.transform(project_data_test['clean_subcategories'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix after one hot encodig \",sub_categories_one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ak', 'al', 'ar', 'az', 'ca', 'co', 'ct', 'dc', 'de', 'fl', 'ga', 'hi', 'ia', 'id', 'il', 'in', 'ks', 'ky', 'la', 'ma', 'md', 'me', 'mi', 'mn', 'mo', 'ms', 'mt', 'nc', 'nd', 'ne', 'nh', 'nj', 'nm', 'nv', 'ny', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', 'ut', 'va', 'vt', 'wa', 'wi', 'wv', 'wy']\n",
      "Shape of matrix after one hot encodig  (87398, 51)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "school_state_one_hot = vectorizer.fit_transform(project_data_train['school_state'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix after one hot encodig \",school_state_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ak', 'al', 'ar', 'az', 'ca', 'co', 'ct', 'dc', 'de', 'fl', 'ga', 'hi', 'ia', 'id', 'il', 'in', 'ks', 'ky', 'la', 'ma', 'md', 'me', 'mi', 'mn', 'mo', 'ms', 'mt', 'nc', 'nd', 'ne', 'nh', 'nj', 'nm', 'nv', 'ny', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', 'ut', 'va', 'vt', 'wa', 'wi', 'wv', 'wy']\n",
      "Shape of matrix after one hot encodig  (21850, 51)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(project_data_train['school_state'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "school_state_one_hot_test = vectorizer.transform(project_data_test['school_state'].values)\n",
    "print(\"Shape of matrix after one hot encodig \",school_state_one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grades_3_5', 'grades_6_8', 'grades_9_12', 'grades_prek_2']\n",
      "Shape of matrix after one hot encodig  (87398, 4)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "gra_cat_one_hot = vectorizer.fit_transform(project_data_train['project_grade_category'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix after one hot encodig \",gra_cat_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grades_3_5', 'grades_6_8', 'grades_9_12', 'grades_prek_2']\n",
      "Shape of matrix after one hot encodig  (21850, 4)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(project_data_train['project_grade_category'].values)\n",
    "print(vectorizer.get_feature_names())\n",
    "gra_cat_one_hot_test = vectorizer.transform(project_data_test['project_grade_category'].values)\n",
    "print(\"Shape of matrix after one hot encodig \",gra_cat_one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', 'mr', 'mrs', 'ms', 'teacher']\n",
      "Shape of matrix after one hot encodig  (87398, 5)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "tea_pre_one_hot = vectorizer.fit_transform(project_data_train['teacher_prefix'].values.astype('str'))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix after one hot encodig \",tea_pre_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dr', 'mr', 'mrs', 'ms', 'teacher']\n",
      "Shape of matrix after one hot encodig  (21850, 5)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(project_data_train['teacher_prefix'].values.astype('str'))\n",
    "tea_pre_one_hot_test = vectorizer.transform(project_data_test['teacher_prefix'].values.astype('str'))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape of matrix after one hot encodig \",tea_pre_one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concating Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87398, 9)\n",
      "(87398, 30)\n",
      "(87398, 51)\n",
      "(87398, 4)\n",
      "(87398, 5)\n",
      "(87398, 2)\n"
     ]
    }
   ],
   "source": [
    "print(categories_one_hot.shape)\n",
    "print(sub_categories_one_hot.shape)\n",
    "print(school_state_one_hot.shape)\n",
    "print(gra_cat_one_hot.shape)\n",
    "print(tea_pre_one_hot.shape)\n",
    "print(remain_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87398, 101)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "concat_train = hstack((categories_one_hot,sub_categories_one_hot,school_state_one_hot,gra_cat_one_hot,tea_pre_one_hot, remain_train))\n",
    "concat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21850, 101)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "concat_test = hstack((categories_one_hot_test,sub_categories_one_hot_test,school_state_one_hot_test,gra_cat_one_hot_test,tea_pre_one_hot_test, remain_test))\n",
    "concat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_array_train = concat_train.toarray()\n",
    "concat_array_test = concat_test.toarray()\n",
    "nrows, ncols = concat_array_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Concat_input (InputLayer)       [(None, 101)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer12 (Embedding)             (None, 101, 16)      1632        Concat_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Text_data (InputLayer)          [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer22 (Conv1D)                (None, 92, 64)       10304       layer12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer11 (Embedding)             (None, 400, 300)     15497100    Text_data[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 46, 64)       0           layer22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer21 (LSTM)                  (None, 100)          160400      layer11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer32 (Conv1D)                (None, 37, 32)       20512       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           layer21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1184)         0           layer32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 1284)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer2 (Dense)                  (None, 512)          657920      concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512)          2048        layer2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer4 (Dense)                  (None, 256)          131328      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        layer4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer5 (Dense)                  (None, 128)          32896       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         layer5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer6 (Dense)                  (None, 64)           8256        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            65          layer6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 16,523,997\n",
      "Trainable params: 1,025,105\n",
      "Non-trainable params: 15,498,892\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Input_model_1 = Input(shape=(max_review_length,) ,name=\"Text_data\")\n",
    "layer11 = Embedding(vocab_size, 300 ,weights=[embedding_matrix],trainable=False ,name=\"layer11\")(Input_model_1)\n",
    "layer21 = LSTM(units = 100,activation='relu',kernel_initializer='he_normal',return_sequences = False,name=\"layer21\")(layer11)\n",
    "layer41 = Flatten()(layer21)\n",
    "\n",
    "Input_model_2 = Input(shape=(ncols,),name=\"Concat_input\")\n",
    "layer12 = Embedding(ncols + 1, 16 ,name=\"layer12\")(Input_model_2)\n",
    "layer22 = Conv1D(64, kernel_size = 10, activation='relu' ,name=\"layer22\")(layer12)\n",
    "pool_1 =  MaxPooling1D(pool_size=2)(layer22)\n",
    "layer32 = Conv1D(32, kernel_size = 10, activation='relu' ,name=\"layer32\")(pool_1)\n",
    "layer42 = Flatten()(layer32)\n",
    "\n",
    "concat_layer = concatenate(inputs=[layer41,layer42],name=\"concat\")\n",
    "\n",
    "layer2 = Dense(units=512,activation='relu',kernel_initializer='he_normal',name=\"layer2\")(concat_layer)\n",
    "norm_1 = BatchNormalization()(layer2)\n",
    "layer3 = Dropout(0.25)(norm_1)\n",
    "layer4 = Dense(units=256,activation='relu',kernel_initializer='he_normal',name=\"layer4\")(layer3)\n",
    "norm_2 = BatchNormalization()(layer4)\n",
    "layer5 = Dense(units=128,activation='relu',kernel_initializer='he_normal',name=\"layer5\")(norm_2)\n",
    "norm_3 = BatchNormalization()(layer5)\n",
    "layer6 = Dense(units=64,activation='relu',kernel_initializer='he_normal',name=\"layer6\")(norm_3)\n",
    "\n",
    "output = Dense(units=1,activation='sigmoid',kernel_initializer=\"glorot_uniform\",name=\"output\")(layer6)\n",
    "\n",
    "model = Model(inputs=[Input_model_1,Input_model_2],outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile \n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics = [\"accuracy\", auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "logs2 = r\"C:\\Users\\Dewang\\Desktop\\AAIC notes\\0.0 Assignments\\AAIC classroom Assignments\\logs\\ex3\"\n",
    "tensorboard = TensorBoard(log_dir = logs2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1,\n",
    "                1: 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87398 samples, validate on 21850 samples\n",
      "Epoch 1/10\n",
      "87398/87398 [==============================] - 546s 6ms/sample - loss: 0.2305 - acc: 0.5756 - auc: 0.5344 - val_loss: 0.6402 - val_acc: 0.8486 - val_auc: 0.5170\n",
      "Epoch 2/10\n",
      "87398/87398 [==============================] - 546s 6ms/sample - loss: 0.2204 - acc: 0.6201 - auc: 0.5768 - val_loss: 0.6936 - val_acc: 0.5209 - val_auc: 0.5219\n",
      "Epoch 3/10\n",
      "87398/87398 [==============================] - 558s 6ms/sample - loss: 0.2169 - acc: 0.6335 - auc: 0.6068 - val_loss: 0.6650 - val_acc: 0.8414 - val_auc: 0.5246\n",
      "Epoch 4/10\n",
      "87398/87398 [==============================] - 527s 6ms/sample - loss: 0.2147 - acc: 0.6387 - auc: 0.6239 - val_loss: 0.6180 - val_acc: 0.8486 - val_auc: 0.5431\n",
      "Epoch 5/10\n",
      "87398/87398 [==============================] - 528s 6ms/sample - loss: 0.2129 - acc: 0.6458 - auc: 0.6352 - val_loss: 0.6631 - val_acc: 0.8078 - val_auc: 0.5630\n",
      "Epoch 6/10\n",
      "87398/87398 [==============================] - 526s 6ms/sample - loss: 0.2112 - acc: 0.6497 - auc: 0.6471 - val_loss: 0.6115 - val_acc: 0.8484 - val_auc: 0.5450\n",
      "Epoch 7/10\n",
      "87398/87398 [==============================] - 525s 6ms/sample - loss: 0.2094 - acc: 0.6563 - auc: 0.6580 - val_loss: 0.5731 - val_acc: 0.8486 - val_auc: 0.5539\n",
      "Epoch 8/10\n",
      "87398/87398 [==============================] - 533s 6ms/sample - loss: 0.2080 - acc: 0.6567 - auc: 0.6660 - val_loss: 0.6101 - val_acc: 0.8333 - val_auc: 0.5605\n",
      "Epoch 9/10\n",
      "87398/87398 [==============================] - 534s 6ms/sample - loss: 0.2068 - acc: 0.6606 - auc: 0.6739 - val_loss: 0.5590 - val_acc: 0.8290 - val_auc: 0.5790\n",
      "Epoch 10/10\n",
      "87398/87398 [==============================] - 546s 6ms/sample - loss: 0.2047 - acc: 0.6665 - auc: 0.6846 - val_loss: 0.5887 - val_acc: 0.8335 - val_auc: 0.5519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2438487e080>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "model.fit([pad_docs_train, concat_array_train],y_train,\n",
    "           batch_size=256,epochs=10,\n",
    "           validation_data= ([pad_docs_test,concat_array_test],y_test),class_weight = class_weights\n",
    "         ,callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"model3.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "json_file = open('model3.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model3.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile \n",
    "loaded_model.compile(optimizer='adam',loss='binary_crossentropy',metrics = [\"accuracy\", auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = loaded_model.evaluate([pad_idf_test,state_enc_test,grade_enc_test,category_test,subcategory_test,prefix_enc_test,remain_test],y_test,\n",
    "           callbacks = [tensorboard])\n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
